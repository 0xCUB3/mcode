# mcode bench job configuration (used to generate a ConfigMap).
#
# Apply/update in OpenShift:
#   oc create configmap mcode-bench-config --from-env-file=deploy/k8s/bench.env -o yaml --dry-run=client | oc apply -f -
#
# Then recreate the Job:
#   oc delete job mcode-bench --ignore-not-found && oc create -f deploy/k8s/mcode-bench-indexed-job.yaml

BENCHMARK=humaneval

# Inference backend
#
# Option A (recommended on this cluster): vLLM via Mellea `openai` backend.
BACKEND=openai
MODEL=ibm-granite/granite-3.0-8b-instruct
OPENAI_BASE_URL=http://vllm:8000/v1
# The OpenAI SDK requires an API key value even for local OpenAI-compatible servers.
OPENAI_API_KEY=dummy
#
# Option B: Ollama (simpler, but can bottleneck at higher shard counts).
# BACKEND=ollama
# MODEL=granite3-dense:8b
# OLLAMA_HOST=http://ollama:11434

# Guardrail: prevent runaway generations from producing enormous outputs (and OOMing pods).
# This maps to Ollama's `num_predict`.
MCODE_MAX_NEW_TOKENS=1024

LOOP_BUDGET=3
TIMEOUT_S=60
STRATEGY=repair

# SOFAI two-tier strategy: set STRATEGY=sofai and configure S2 model.
# S2_MODEL=granite4:32b
# S2_BACKEND=ollama
# S2_MODE=best_attempt

# Must match `spec.completions` in the Job manifest.
SHARD_COUNT=5

# Optional: how many shards to run at once (defaults to SHARD_COUNT).
# PARALLELISM=5

# Optional: override the image used by the Job. If unset, `deploy/k8s/run-bench.sh` defaults to:
#   image-registry.openshift-image-registry.svc:5000/<current-namespace>/mcode:latest
# MCODE_IMAGE=icr.io/<namespace>/mcode:latest

# Optional: run only first N tasks.
# LIMIT=10
